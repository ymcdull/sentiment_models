{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## This notebook is just a collection of some baseline sentiment models\n# Most code is coming from https://github.com/abdulfatir/twitter-sentiment-analysis\n# Dataset is from Kaggle: https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n# Use small portion of data for quick test, no feature engineering included","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Sklearn\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# Keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding, Flatten\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom xgboost import XGBClassifier","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use small portion of data for quick test\ndf = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv\")\n\ntrain_df = df.iloc[:3000]\nvalid_df = df.iloc[3000:4000].reset_index(drop=True)","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\n\ntfidf_train = tfidf.fit_transform(train_df.tweet.values)\ntfidf_valid = tfidf.transform(valid_df.tweet.values)","execution_count":72,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearSVC(C=0.1)\nmodel.fit(tfidf_train, train_df.label)\n\ny_pred = model.predict(tfidf_valid)\nprint(classification_report(valid_df.label, y_pred))","execution_count":73,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96       925\n           1       1.00      0.05      0.10        75\n\n    accuracy                           0.93      1000\n   macro avg       0.96      0.53      0.53      1000\nweighted avg       0.93      0.93      0.90      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB()\nmodel.partial_fit(tfidf_train, train_df.label, classes=[0, 1])\n\ny_pred = model.predict(tfidf_valid)\nprint(classification_report(valid_df.label, y_pred))","execution_count":74,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96       925\n           1       1.00      0.07      0.12        75\n\n    accuracy                           0.93      1000\n   macro avg       0.96      0.53      0.54      1000\nweighted avg       0.93      0.93      0.90      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(tfidf_train, train_df.label)\n\ny_pred = model.predict(tfidf_valid)\nprint(classification_report(valid_df.label, y_pred))","execution_count":75,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.94      1.00      0.97       925\n           1       1.00      0.15      0.26        75\n\n    accuracy                           0.94      1000\n   macro avg       0.97      0.57      0.61      1000\nweighted avg       0.94      0.94      0.91      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(tfidf_train, train_df.label)\n\ny_pred = model.predict(tfidf_valid)\nprint(classification_report(valid_df.label, y_pred))","execution_count":76,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96       925\n           1       1.00      0.05      0.10        75\n\n    accuracy                           0.93      1000\n   macro avg       0.96      0.53      0.53      1000\nweighted avg       0.93      0.93      0.90      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression (keras)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_dim):\n    model = Sequential()\n    model.add(Dense(1, input_dim=input_dim, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = build_model(len(tfidf.vocabulary_))\nmodel.fit(tfidf_train.toarray(), train_df.label, epochs=1)\n\ny_pred = model.predict(tfidf_valid.toarray())\nprint(classification_report(valid_df.label, y_pred.ravel() > 0.5))","execution_count":77,"outputs":[{"output_type":"stream","text":"94/94 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.8578\n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96       925\n           1       0.00      0.00      0.00        75\n\n    accuracy                           0.93      1000\n   macro avg       0.46      0.50      0.48      1000\nweighted avg       0.86      0.93      0.89      1000\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\nmodel.fit(tfidf_train, train_df.label)\n\ny_pred = model.predict(tfidf_valid)\nprint(classification_report(valid_df.label, y_pred))","execution_count":78,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"[20:40:14] WARNING: ../src/learner.cc:541: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[20:40:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n              precision    recall  f1-score   support\n\n           0       0.94      0.99      0.96       925\n           1       0.67      0.16      0.26        75\n\n    accuracy                           0.93      1000\n   macro avg       0.80      0.58      0.61      1000\nweighted avg       0.92      0.93      0.91      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# DenseNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def build_model():\n#     model = Sequential()\n#     model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n#     model.add(Dense(1, activation='sigmoid'))\n#     model.compile(loss='binary_crossentropy',\n#                   optimizer='adam', metrics=['accuracy'])\n#     return model","execution_count":79,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://github.com/abdulfatir/twitter-sentiment-analysis/blob/master/code/cnn.py\n\nGLOVE_FILE = \"/kaggle/input/glove6b/glove.6B.50d.txt\"\n\ndef get_glove_vectors(vocab):\n    \"\"\"\n    Extracts glove vectors from seed file only for words present in vocab.\n    \"\"\"\n    glove_vectors = {}\n    with open(GLOVE_FILE, 'r') as glove_file:\n        for i, line in enumerate(glove_file):\n            tokens = line.strip().split()\n            word = tokens[0]\n            if vocab.get(word):\n                vector = [float(e) for e in tokens[1:]]\n                glove_vectors[word] = np.array(vector)\n    return glove_vectors\n\ndef build_model(vocab_size, kernel_size, dim, embedding_matrix, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n    model.add(Dropout(0.4))\n    model.add(Conv1D(600, kernel_size, padding='valid', activation='relu', strides=1))\n    model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n    model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n    model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n    model.add(Flatten())\n    model.add(Dense(600))\n    model.add(Dropout(0.5))\n    model.add(Activation('relu'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef get_feature_vector(text, vocab):\n    \"\"\"\n    Generates a feature vector for each tweet where each word is\n    represented by integer index based on rank in vocabulary.\n    \"\"\"\n    words = text.split()\n    feature_vector = []\n    for i in range(len(words) - 1):\n        word = words[i]\n        if vocab.get(word) is not None:\n            feature_vector.append(vocab.get(word))\n    if len(words) >= 1:\n        if vocab.get(words[-1]) is not None:\n            feature_vector.append(vocab.get(words[-1]))\n    return feature_vector\n\ndef process_texts(texts, vocab):\n    vecs = []\n    for text in texts:\n        feature_vector = get_feature_vector(text, vocab)\n        vecs.append(feature_vector)\n    return vecs","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_size = 3\ndim = 50\nmax_length = 64\n\nvocab = tfidf.vocabulary_\nvocab_size = len(vocab)\n\nglove_vectors = get_glove_vectors(vocab)\nembedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n\nfor word, i in vocab.items():\n    glove_vector = glove_vectors.get(word)\n    if glove_vector is not None:\n        embedding_matrix[i] = glove_vector","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vecs = process_texts(train_df.tweet.values, vocab)\nvalid_vecs = process_texts(valid_df.tweet.values, vocab)\n\ntrain_vecs = pad_sequences(train_vecs, maxlen=max_length, padding='post')\nvalid_vecs = pad_sequences(valid_vecs, maxlen=max_length, padding='post')\n\nmodel = build_model(vocab_size, kernel_size, dim, embedding_matrix, max_length)\n\n# filepath = \"./models/4cnn-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n# checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\nmodel.fit(train_vecs, train_df.label, batch_size=128, epochs=8, validation_split=0.1, shuffle=True, callbacks=[reduce_lr])","execution_count":82,"outputs":[{"output_type":"stream","text":"Epoch 1/8\n22/22 [==============================] - 1s 32ms/step - loss: 0.3721 - accuracy: 0.9175 - val_loss: 0.2611 - val_accuracy: 0.9267\nEpoch 2/8\n22/22 [==============================] - 0s 23ms/step - loss: 0.2375 - accuracy: 0.9372 - val_loss: 0.2541 - val_accuracy: 0.9267\nEpoch 3/8\n22/22 [==============================] - 1s 28ms/step - loss: 0.2324 - accuracy: 0.9336 - val_loss: 0.2549 - val_accuracy: 0.9267\nEpoch 4/8\n22/22 [==============================] - 0s 21ms/step - loss: 0.2182 - accuracy: 0.9322 - val_loss: 0.2348 - val_accuracy: 0.9267\nEpoch 5/8\n22/22 [==============================] - 0s 21ms/step - loss: 0.2156 - accuracy: 0.9304 - val_loss: 0.2123 - val_accuracy: 0.9267\nEpoch 6/8\n22/22 [==============================] - 0s 21ms/step - loss: 0.1845 - accuracy: 0.9359 - val_loss: 0.2427 - val_accuracy: 0.9267\nEpoch 7/8\n22/22 [==============================] - 1s 29ms/step - loss: 0.1640 - accuracy: 0.9398 - val_loss: 0.2227 - val_accuracy: 0.9267\nEpoch 8/8\n22/22 [==============================] - 1s 26ms/step - loss: 0.1467 - accuracy: 0.9447 - val_loss: 0.2116 - val_accuracy: 0.9367\n","name":"stdout"},{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f90471f1910>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(valid_vecs)\nprint(classification_report(valid_df.label, y_pred.ravel() > 0.5))","execution_count":83,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96       925\n           1       0.38      0.07      0.11        75\n\n    accuracy                           0.92      1000\n   macro avg       0.66      0.53      0.54      1000\nweighted avg       0.89      0.92      0.90      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, dim, embedding_matrix, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n    model.add(Dropout(0.4))\n    model.add(LSTM(128))\n    model.add(Dense(64))\n    model.add(Dropout(0.5))\n    model.add(Activation('relu'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(vocab_size, dim, embedding_matrix, max_length)","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Val_accuracy does not go up???\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\nmodel.fit(train_vecs, train_df.label, batch_size=32, epochs=8, validation_split=0.1, shuffle=True, callbacks=[reduce_lr])","execution_count":86,"outputs":[{"output_type":"stream","text":"Epoch 1/8\n85/85 [==============================] - 3s 17ms/step - loss: 0.3906 - accuracy: 0.8995 - val_loss: 0.2632 - val_accuracy: 0.9267\nEpoch 2/8\n85/85 [==============================] - 1s 12ms/step - loss: 0.2602 - accuracy: 0.9388 - val_loss: 0.2629 - val_accuracy: 0.9267\nEpoch 3/8\n85/85 [==============================] - 1s 12ms/step - loss: 0.2893 - accuracy: 0.9245 - val_loss: 0.2649 - val_accuracy: 0.9267\nEpoch 4/8\n85/85 [==============================] - 1s 12ms/step - loss: 0.2646 - accuracy: 0.9315 - val_loss: 0.2623 - val_accuracy: 0.9267\nEpoch 5/8\n85/85 [==============================] - 1s 13ms/step - loss: 0.2541 - accuracy: 0.9332 - val_loss: 0.2506 - val_accuracy: 0.9267\nEpoch 6/8\n85/85 [==============================] - 1s 14ms/step - loss: 0.2614 - accuracy: 0.9353 - val_loss: 0.2668 - val_accuracy: 0.9267\nEpoch 7/8\n85/85 [==============================] - 1s 12ms/step - loss: 0.2700 - accuracy: 0.9296 - val_loss: 0.2661 - val_accuracy: 0.9267\nEpoch 8/8\n85/85 [==============================] - 1s 12ms/step - loss: 0.2872 - accuracy: 0.9214 - val_loss: 0.2628 - val_accuracy: 0.9267\n","name":"stdout"},{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f8f4437b8d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(valid_vecs)\nprint(classification_report(valid_df.label, y_pred.ravel() > 0.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT's family"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install simpletransformers","execution_count":36,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: simpletransformers in /opt/conda/lib/python3.7/site-packages (0.60.6)\nRequirement already satisfied: transformers>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.2.2)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.9.4)\nRequirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.55.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.4.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2020.11.13)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.10.15)\nRequirement already satisfied: streamlit in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.76.0)\nRequirement already satisfied: tensorboardx in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.23.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.25.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.1.95)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.1.5)\nRequirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.19.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (20.8)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (0.0.43)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (3.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=4.2.0->simpletransformers) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2019.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->simpletransformers) (1.15.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.2.0->simpletransformers) (1.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.2.0->simpletransformers) (7.1.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (2.1.0)\nRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (3.14.0)\nRequirement already satisfied: tzlocal in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (2.1)\nRequirement already satisfied: blinker in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.4)\nRequirement already satisfied: tornado>=5.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (5.0.2)\nRequirement already satisfied: watchdog in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.10.4)\nRequirement already satisfied: altair>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.1.0)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.0.1)\nRequirement already satisfied: gitpython in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (3.1.12)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (7.2.0)\nRequirement already satisfied: base58 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (2.1.0)\nRequirement already satisfied: cachetools>=4.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.1.1)\nRequirement already satisfied: validators in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.18.2)\nRequirement already satisfied: astor in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.8.1)\nRequirement already satisfied: pydeck>=0.1.dev5 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.5.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.10.2)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.2.0)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.2)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\nRequirement already satisfied: ipywidgets>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.2)\nRequirement already satisfied: traitlets>=4.3.2 in /opt/conda/lib/python3.7/site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.5)\nRequirement already satisfied: ipykernel>=5.1.2 in /opt/conda/lib/python3.7/site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.3)\nRequirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.1.7)\nRequirement already satisfied: ipython>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.19.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.8)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (49.6.0.post20201009)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.7.3)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.0)\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\nRequirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.8)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1)\nRequirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.0)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair>=3.2.0->streamlit->simpletransformers) (20.3.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair>=3.2.0->streamlit->simpletransformers) (0.17.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\nRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (20.0.0)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.0.7)\nRequirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\nRequirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython->streamlit->simpletransformers) (4.0.5)\nRequirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit->simpletransformers) (3.0.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.2)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.2.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\nRequirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.10)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\nRequirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (0.19.5)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.0.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (0.4.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.8.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.3.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (1.0.1)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (3.5.4)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (2.3)\nRequirement already satisfied: pathtools>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from watchdog->streamlit->simpletransformers) (0.1.2)\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.rename(columns={'tweet': 'text', 'label': 'labels'})\nvalid_df = valid_df.rename(columns={'tweet': 'text', 'label': 'labels'})","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DistilBERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = ClassificationArgs(num_train_epochs=3, overwrite_output_dir=True, train_batch_size=16)\nmodel = ClassificationModel(\n    \"distilbert\", \"distilbert-base-uncased\", args=model_args\n)\nmodel.train_model(train_df)\npredictions, raw_outputs = model.predict(valid_df.text.values)\nprint(classification_report(valid_df.labels, predictions))","execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a13aa0f9b4243a79203d0bf0fc1c2d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fec850a9b7aa4db09285031de2898205"}},"metadata":{}},{"output_type":"stream","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b7da0809ad42d7b56a97f45d465571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22190618cf504020afabd2e3bbe97f89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267af7a94831483abe42582e10687adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1236ebf7e64d778d4eb1ddad0105af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e60a87f434fb452fbcc95dbb937606fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a81daee8ba4d7f977bb212b33f375f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8161e218c034cf5b42ffac310195836"}},"metadata":{}},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       925\n           1       0.67      0.56      0.61        75\n\n    accuracy                           0.95      1000\n   macro avg       0.82      0.77      0.79      1000\nweighted avg       0.94      0.95      0.94      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = ClassificationArgs(num_train_epochs=3, overwrite_output_dir=True, train_batch_size=16)\nmodel = ClassificationModel(\n    \"bert\", \"bert-base-uncased\", args=model_args\n)","execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a003f9414fc844e4ab4697ad3e37bc1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1c10b4751b40b38f6556e7d7ece037"}},"metadata":{}},{"output_type":"stream","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbeecfe0ed454454b8c509b02c18db92"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train_model(train_df)\npredictions, raw_outputs = model.predict(valid_df.text.values)\nprint(classification_report(valid_df.labels, predictions))","execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a619f22c9fd40538ea3104648c0d6bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"567373f1509d42a4b8f8f575ebb59d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45a0e16f3de43f1bdde6c75806d6da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d92dabd92b94f0dabde7a49d56d0001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a298e970abfb4714b7e9d8044e34dc27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16bf19fbe34f4118b70a7adb679a9be6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42680302a7f34dbaad69bc473c3f5fa3"}},"metadata":{}},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       925\n           1       0.67      0.51      0.58        75\n\n    accuracy                           0.94      1000\n   macro avg       0.81      0.74      0.77      1000\nweighted avg       0.94      0.94      0.94      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Roberta-base"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = ClassificationArgs(num_train_epochs=3, overwrite_output_dir=True, train_batch_size=16)\nmodel = ClassificationModel(\n    \"roberta\", \"roberta-base\", args=model_args\n)","execution_count":39,"outputs":[{"output_type":"stream","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nmodel.train_model(train_df)","execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb46da965d844981ae445c3055115bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a2fcf68942420d9ff5738bda3dfe5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0110c498a1a14c8da67bf024849ba6d8"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2683f320362489eb67a83f2518061cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d8580e589d43c9a4131fa4da1d6596"}},"metadata":{}},{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"(564, 0.14492184107127049)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions, raw_outputs = model.predict(valid_df.text.values)","execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86fd985b6394ae889e7121ba9ff267e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2098c47cc049aeac70b966cac202af"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(valid_df.labels, predictions))","execution_count":53,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       925\n           1       0.77      0.65      0.71        75\n\n    accuracy                           0.96      1000\n   macro avg       0.87      0.82      0.84      1000\nweighted avg       0.96      0.96      0.96      1000\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# XLNET"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_args = ClassificationArgs(num_train_epochs=3, overwrite_output_dir=True, train_batch_size=16)\nmodel = ClassificationModel(\n    \"xlnet\", \"xlnet-base-cased\", args=model_args\n)\nmodel.train_model(train_df)\npredictions, raw_outputs = model.predict(valid_df.text.values)\nprint(classification_report(valid_df.labels, predictions))","execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87125c521b843b3b89128fa13af320d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f5a57124ad4dfc816181e430df9212"}},"metadata":{}},{"output_type":"stream","text":"Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f97cb83d7f6498f8349a72fe362165b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91697f87b7fa4833b53705e48d058982"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b626f80226c456c9c798e7f558c01db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728760efa4b645ca8d07b4c41d25ed42"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97ddcd602984872bfc3a6b5219636f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 3:   0%|          | 0/188 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54013aa51ba94e53ac62114a4c2d6ad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728157550ad54794a36b7c32565033f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b8dbfe3b9145d585a75e22a53836c9"}},"metadata":{}},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97       925\n           1       0.65      0.40      0.50        75\n\n    accuracy                           0.94      1000\n   macro avg       0.80      0.69      0.73      1000\nweighted avg       0.93      0.94      0.93      1000\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}